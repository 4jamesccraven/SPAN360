#!/usr/bin/env python3

import re
import os
import warnings
import logging
from argparse import ArgumentParser, Namespace
from collections.abc import Callable

import requests
from bs4 import BeautifulSoup
from urllib3.exceptions import InsecureRequestWarning
from tqdm import tqdm

# We're just scraping text, no need to verify
warnings.simplefilter('ignore', InsecureRequestWarning)

# Initialise logging
LOGGER = logging.getLogger('hsms')
LOGGER.setLevel(logging.DEBUG)
file_handler = logging.FileHandler('hsms.log', mode='w')
file_handler.setFormatter(
    logging.Formatter('%(name)s [%(levelname)s]: %(message)s')
)
LOGGER.addHandler(file_handler)


def cli() -> Namespace:
    parser = ArgumentParser('hsms',
                            description='util to collect text from HSMS')
    parser.set_defaults(parser=parser)
    subps = parser.add_subparsers(title='action',
                                  required=True)

    gather = subps.add_parser('gather')
    gather.set_defaults(task=gather_task)

    clean = subps.add_parser('clean')
    clean.set_defaults(task=clean_task)
    clean.add_argument('--compare',
                       help='Compare the changes to the text before and'
                            ' after the cleaning.',
                       action='store_true')

    return parser.parse_args()


def gather_task(args: Namespace):
    try:
        with open('texts.in', 'r', encoding='utf8') as f:
            texts = [line.strip() for line in f]
    except FileNotFoundError:
        args.parser.exit(message='hsms: error: Input file not found. Must specify a file'
                         ' called `texts.in`.')
        return

    this_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(this_dir, 'unproccessed_text')
    if not os.path.isdir(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    file_map: dict[str, int] = {}
    for text in tqdm(texts, maxinterval=1.0):
        url = f'https://www.hispanicseminary.org/{text}'

        response = requests.get(url, verify=False)

        if response.status_code != 200:
            LOGGER.error(f'{url}')
            return

        soup = BeautifulSoup(response.content, 'html.parser')

        text_body = soup.find(id='selectable')

        if text_body is None:
            LOGGER.error(f'{url}')
            return

        text_body = [line.strip() for line in text_body.text.split('\n')[1:]]
        match = re.fullmatch(r'\{RMK: (.*)\.\}', text_body[0])
        title = match.group(1) if match else text.split('/')[1]
        title = title.replace(' ', '_')

        file_map[title] = file_map.get(title, 0) + 1

        write_path = os.path.join(output_dir, title + str(file_map[title]) + '.txt')
        LOGGER.debug(write_path)
        try:
            with open(write_path, 'w', encoding='utf8') as f:
                f.write('\n'.join(text_body))
        except Exception as e:
            LOGGER.error(f'{title} {e}')


def strfpath(path: str) -> str:
    with open(path, 'r', encoding='utf8') as f:
        return '\n'.join([line.strip() for line in f])


# Left in global to be imported elsewhere
CB_PAT = re.compile(r'\{CB\d+\.(.*?)\}', re.DOTALL)
IN_PAT = re.compile(r'\{IN(.*?)\.\}? ?', re.DOTALL)
CW_PAT = re.compile(r'\{CW. (.*?) ?\}')

def clean_task(args: Namespace):
    files = os.listdir('unproccessed_text')
    files = [os.path.join('unproccessed_text', file) for file in files]
    files = [os.path.abspath(file) for file in files]

    text = '\n'.join(strfpath(file) for file in files)

    def take_group(x):
        whole_match = x.group(0)
        kept = x.group(1)
        LOGGER.info(whole_match.replace(kept, ''))
        return kept

    def rm_and_log(x):
        LOGGER.info(x.group(0))
        return ''

    SUBSTITUTIONS: dict[re.Pattern, str | Callable] = {
        CB_PAT: take_group,
        IN_PAT: rm_and_log,
        CW_PAT: take_group,
    }

    cleaned_text = text
    for pattern, replacement in SUBSTITUTIONS.items():
        cleaned_text = re.sub(pattern, replacement, cleaned_text)

    try:
        if args.compare:
            for raw, clean in zip(text.splitlines(), cleaned_text.splitlines()):
                print(f'{raw[:99]:<{99}} | {clean}')
        else:
            print(cleaned_text)
    except BrokenPipeError:
        ...


def main() -> None:
    args = cli()
    args.task(args)


if __name__ == '__main__':
    main()

