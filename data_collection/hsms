#!/usr/bin/env python3

import re
import os
import warnings
import logging
from argparse import ArgumentParser, Namespace
from collections.abc import Callable
from functools import partial
from sys import stderr

import requests
from bs4 import BeautifulSoup
from urllib3.exceptions import InsecureRequestWarning
from tqdm import tqdm

# We're just scraping text, no need to verify
warnings.simplefilter('ignore', InsecureRequestWarning)

# Initialise logging
LOGGER = logging.getLogger('hsms')
LOGGER.setLevel(logging.DEBUG)
file_handler = logging.FileHandler('hsms.log', mode='w')
file_handler.setFormatter(
    logging.Formatter('%(name)s [%(levelname)s]: %(message)s')
)
LOGGER.addHandler(file_handler)


tqdm_wrapper = partial(tqdm,
                       bar_format='{percentage:3.0f}% [{bar}] {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
                       ascii='-=',
                       maxinterval=1.0,
                       file=stderr)


def cli() -> Namespace:
    parser = ArgumentParser('hsms',
                            description='util to collect text from HSMS')
    parser.set_defaults(parser=parser)
    subps = parser.add_subparsers(title='action',
                                  required=True)

    gather = subps.add_parser('gather',
                              help='Gather texts specified in texts.in'
                                   ' from the Digital Library of the'
                                   ' HSMS')
    gather.set_defaults(task=gather_task)

    clean = subps.add_parser('clean',
                             help='Clean the data gathered by `gather`'
                                  ' and print the data to stdout.'
                                  ' redirect stdout to a file to save.')
    clean.set_defaults(task=clean_task)
    clean.add_argument('--compare',
                       help='Compare the changes to the text before and'
                            ' after the cleaning.',
                       action='store_true')
    clean.add_argument('-s', '--show_progress',
                       help='Print a progress bar to stderr.'
                            ' not recommended for saving final'
                            ' output.',
                       action='store_true')

    return parser.parse_args()


def gather_task(args: Namespace):
    try:
        with open('texts.in', 'r', encoding='utf8') as f:
            texts = [line.strip() for line in f if line.strip()[0] != '#']
    except FileNotFoundError:
        args.parser.exit(message='hsms: error: Input file not found. Must specify a file'
                         ' called `texts.in`.')
        return

    this_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(this_dir, 'unproccessed_text')
    if not os.path.isdir(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    file_map: dict[str, int] = {}
    for text in tqdm_wrapper(texts):
        url = f'https://www.hispanicseminary.org/{text}'

        response = requests.get(url, verify=False)

        if response.status_code != 200:
            LOGGER.error(f'{url}')
            return

        soup = BeautifulSoup(response.content, 'html.parser')

        text_body = soup.find(id='selectable')

        if text_body is None:
            LOGGER.error(f'{url}')
            return

        text_body = [line.strip() for line in text_body.text.split('\n')[1:]]
        match = re.fullmatch(r'\{RMK: (.*)\.\}', text_body[0])
        title = match.group(1) if match else text.split('/')[1]
        title = title.replace(' ', '_')

        file_map[title] = file_map.get(title, 0) + 1

        write_path = os.path.join(output_dir, title + str(file_map[title]) + '.txt')
        LOGGER.debug(write_path)
        try:
            with open(write_path, 'w', encoding='utf8') as f:
                f.write('\n'.join(text_body))
        except Exception as e:
            LOGGER.error(f'{title} {e}')


def strfpath(path: str) -> str:
    with open(path, 'r', encoding='utf8') as f:
        return '\n'.join([line.strip() for line in f])


# Note: in global scope to be easily imported elsewhere
FL_PAT = re.compile(r'\[fol\..*?\]')
CB_PAT = re.compile(r'\{CB\d*\.(.*?)\}?', re.DOTALL)
RB_PAT = re.compile(r'\{RUB[.:] ?(.*?) ?\}?', re.DOTALL)
MN_PAT = re.compile(r'\{=?MIN=?[.:]?(.*?)\}', re.DOTALL)
LT_PAT = re.compile(r' ?\{LAT\. ?(.*?) ?\}', re.DOTALL)
IN_PAT = re.compile(r'\{IN(.*?)\.\}? ?', re.DOTALL)
IL_PAT = re.compile(r'\{=?ILL=?\.(.*?)\}', re.DOTALL)
BK_PAT = re.compile(r' ?[\[{]BLNK[.:](.*?)[}\]]?')
CW_PAT = re.compile(r'\{CW. (.*?) ?\}')
HD_PAT = re.compile(r'\{HD\d*\.(.*?)\}', re.DOTALL)
DG_PAT = re.compile(r'\{=?DIAG=?\.?(.*?)\}', re.DOTALL)
RK_PAT = re.compile(r'\{RMK:(.*?)\}', re.DOTALL)
SY_PAT = re.compile(r'\{SYMB[.:](.*?)\}', re.DOTALL)
GL_PAT = re.compile(r'\{GL\.? ? (.*?)\}', re.DOTALL)
AD_PAT = re.compile(r'\{AD\.? ? (.*?)\}', re.DOTALL)
GR_PAT = re.compile(r'\{GRK\.? ? (.*?)\}', re.DOTALL)
BR_PAT = re.compile(r'[{}]')
AN_PAT = re.compile(r'<(.*?)>', re.DOTALL)
CL_PAT = re.compile(r'% ?')
BC_PAT = re.compile(r'\[%\]')
ET_PAT = re.compile(r'&')

def clean_task(args: Namespace):
    orig_files = os.listdir('unproccessed_text')
    files = [os.path.join('unproccessed_text', file) for file in orig_files]
    files = [os.path.abspath(file) for file in files]

    destinations = [os.path.join('cleaned_data', path) for path in orig_files]

    text = {dest: strfpath(path) for dest, path in zip(destinations, files)}

    def take_group(x):
        whole_match = x.group(0)
        kept = x.group(1).strip()
        LOGGER.info(whole_match.replace(kept, ''))
        return kept

    def rm_and_log(x):
        LOGGER.info(x.group(0).replace('\n', ' '))
        return ''

    SUBSTITUTIONS: dict[re.Pattern, str | Callable] = {
        FL_PAT: rm_and_log,
        CB_PAT: take_group,
        RB_PAT: take_group,
        MN_PAT: rm_and_log, # 1
        LT_PAT: rm_and_log,
        IN_PAT: rm_and_log,
        IL_PAT: take_group, # 2
        BK_PAT: take_group,
        CW_PAT: take_group,
        HD_PAT: rm_and_log,
        DG_PAT: rm_and_log,
        RK_PAT: rm_and_log,
        SY_PAT: rm_and_log,
        GL_PAT: rm_and_log,
        AD_PAT: rm_and_log,
        GR_PAT: rm_and_log,
        BR_PAT: rm_and_log,
        AN_PAT: take_group,
        CL_PAT: rm_and_log,
        BC_PAT: rm_and_log,
        # ET_PAT: 'et',       # 3
    }
    # ^1 miniature not kept because it is unlikely that it will be usable for translation
    # ^2 I have reservations about the usefulness of this data, but I will retain it for
    #    now nonetheless
    # ^3 I will reconsider this change once I have decided if/how I will use this data

    if not os.path.isdir('cleaned_data'):
        os.mkdir('cleaned_data')

    for name, raw_text in text.items():
        cleaned_text = raw_text
        items = SUBSTITUTIONS.items() if not args.show_progress \
            else tqdm_wrapper(SUBSTITUTIONS.items())

        for pattern, replacement in items:
            LOGGER.info(f'### BEGIN {pattern} ###')
            cleaned_text = re.sub(pattern, replacement, cleaned_text)

        cleaned_text = '\n'.join([line for line in cleaned_text.splitlines() if line]) \
                           .replace('-\n', '')

        try:
            if args.compare:
                for raw, clean in zip(raw_text.splitlines(), cleaned_text.splitlines()):
                    print(f'{raw[:99]:<{99}} | {clean}')
            else:
                with open(name, 'w', encoding='utf8') as f:
                    f.write(cleaned_text)

        except BrokenPipeError:
            ...


def main() -> None:
    args = cli()
    args.task(args)


if __name__ == '__main__':
    main()

